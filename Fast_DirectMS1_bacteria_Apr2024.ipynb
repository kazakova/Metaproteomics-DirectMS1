{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyteomics import fasta, mass, parser, cmass, auxiliary as aux\n",
    "from scipy.stats import binom\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "from os import path, listdir, environ\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; \n",
    "import ete3\n",
    "from ete3 import NCBITaxa\n",
    "ncbi = NCBITaxa()\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sf_all(v, n, p):\n",
    "    sf_values = -np.log10(binom.sf(v-1, n, p))\n",
    "    sf_values[np.isinf(sf_values)] = max(sf_values[~np.isinf(sf_values)])\n",
    "    return sf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to folders with sprot and uniprot databases generated in previous Notebook\n",
    "\n",
    "path_to_fasta_dir_uniprot = '/home/kae-13-1/fasta/bacts_bases_uniprot/'\n",
    "path_to_fasta_dir_sprot = '/home/kae-13-1/fasta/bacts_bases_sprot/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742dcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files for preliminary search which should be generated only once\n",
    "outfasta_path = '/home/fasta/prots_10_perc_23012024.fasta'\n",
    "path_to_output_prot_set = '/home/fasta/prots_10_perc_peptides_mz_23012024.pickle'\n",
    "path_to_output_specmap_id = '/home/fasta/specmapid_23012024.pickle'\n",
    "path_to_output_cnt_to_spec = '/home/fasta/cnt_to_spec_23012024.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e860d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to dictionaries generated in previous Notebook\n",
    "with open('/home/kae-13-1/bact_Fast_search/dictionaries/species_leader_uniprot.pickle', 'rb') as f:\n",
    "    species_leader_uniprot = pickle.load(f) # txid for species -> txid group leader by uniprot\n",
    "with open('/home/kae-13-1/bact_Fast_search/dictionaries/species_leader_sprot.pickle', 'rb') as f:\n",
    "    species_leader_sprot = pickle.load(f) # txid for species -> txid group leader by sprot\n",
    "with open('/home/kae-13-1/bact_Fast_search/dictionaries/org_len_sprot.pickle', 'rb') as f:\n",
    "    len_sprot = pickle.load(f) # txid -> the number of sprot proteins\n",
    "with open('/home/kae-13-1/bact_Fast_search/dictionaries/org_len_uniprot.pickle', 'rb') as f:\n",
    "    len_uniprot = pickle.load(f) #txid -> the number of uniprot proteins\n",
    "with open('/home/kae-13-1/bact_Fast_search/dictionaries/exclude_names_set.pickle', 'rb') as f:\n",
    "    exclude_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070233e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE 10% of DATABASE. Should be done only once\n",
    "\n",
    "%%time\n",
    "random_dict = {}\n",
    "for k in len_uniprot.keys():\n",
    "    random_dict[k] = 2000 / len_uniprot[k]\n",
    "    \n",
    "prots = []\n",
    "\n",
    "outf = open(outfasta_path, 'w')\n",
    "outf.close()\n",
    "\n",
    "leaders = set(species_leader_sprot.values()).union(set(species_leader_uniprot.values()))\n",
    "for f in leaders:\n",
    "    prots = [] \n",
    "    if f in species_leader_sprot.values():\n",
    "        n_prot = len_sprot[f]\n",
    "    else: \n",
    "        n_prot = len_uniprot[f]\n",
    "    if n_prot >= 200:\n",
    "        for p in fasta.read(path.join(path_to_fasta_dir_uniprot, str(f) + '.fasta')):\n",
    "            if p[0][:2] == 'sp' or random_dict[f] >= random.random():\n",
    "                prots.append(p)\n",
    "        fasta.write(prots, output=open(outfasta_path, 'a')).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d684e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare protein set. Should be done only once\n",
    "\n",
    "%%time\n",
    "\n",
    "# Add fixed modifications here\n",
    "\n",
    "aa_mass = copy(mass.std_aa_mass)\n",
    "aa_mass['C'] += 57.021464\n",
    "aa_mass\n",
    "\n",
    "prot_sets = defaultdict(list)\n",
    "spec_map_id = dict()\n",
    "cnt_to_spec = list()\n",
    "spec_map_id_max = 0\n",
    "spec_map_id_max_decoy = 0\n",
    "\n",
    "cnt = 0\n",
    "for p in fasta.read(outfasta_path):   \n",
    "    \n",
    "    if 'DECOY_' not in p[0]:\n",
    "        cnt += 1\n",
    "        if cnt % 1000000 == 0:\n",
    "            print(cnt)\n",
    "\n",
    "        decoy_flag = p[0].startswith('DECOY_')\n",
    "\n",
    "        spec = ('DECOY_' if decoy_flag else '') + p[0].split('OX=')[-1].split(' ')[0]\n",
    "        if spec not in spec_map_id:\n",
    "            if not decoy_flag: \n",
    "                spec_map_id_max += 1\n",
    "                spec_map_id[spec] = spec_map_id_max\n",
    "            else:\n",
    "                spec_map_id_max_decoy -= 1\n",
    "                spec_map_id[spec] = spec_map_id_max_decoy\n",
    "        \n",
    "        spec_id = spec_map_id[spec]\n",
    "        cnt_to_spec.append(spec_id)\n",
    "        \n",
    "        peptides = parser.cleave(p[1], '[RK]', 0, min_length=9)\n",
    "        mz_list = []\n",
    "\n",
    "        dont_use_fast_valid = parser.fast_valid(p[1])\n",
    "        for pep in peptides:\n",
    "            plen = len(pep)        \n",
    "            if plen <= 15:\n",
    "                if dont_use_fast_valid or parser.fast_valid(pep):\n",
    "                    mz_list.append(cmass.fast_mass(pep, aa_mass=aa_mass))\n",
    "        for mz in set(mz_list):\n",
    "            prot_sets[mz].append(cnt)\n",
    "                \n",
    "pickle.dump(prot_sets, open(path_to_output_prot_set, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(spec_map_id, open(path_to_output_specmap_id, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(cnt_to_spec, open(path_to_output_cnt_to_spec, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd581736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4c670",
   "metadata": {},
   "source": [
    "# RESTART NOTEBOOK HERE IF prot_sets were just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_accuracy = 4.0 # (in ppm)\n",
    "mz_for_mass_accuracy = 1000 # (approximate max mz value)\n",
    "mz_step = mass_accuracy * 1e-6 * mz_for_mass_accuracy\n",
    "mz_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_output_prot_set, 'rb') as handle:\n",
    "    prot_sets = pickle.load(handle)\n",
    "spec_map_id = pickle.load(open(path_to_output_specmap_id, 'rb'))\n",
    "cnt_to_spec= pickle.load(open(path_to_output_cnt_to_spec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_map_id_reversed = dict()\n",
    "for k, v in spec_map_id.items():\n",
    "    spec_map_id_reversed[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fe27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "protsN = Counter()\n",
    "\n",
    "accurate_mz_map = defaultdict(list)\n",
    "\n",
    "for v, prots in prot_sets.items():\n",
    "    v_int = int(v/mz_step)\n",
    "    accurate_mz_map[v_int].append(v)\n",
    "    protsN.update(prots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(df1, custom_range_mass_accuracy, score_threshold = 4.0):\n",
    "    prots_spc = Counter()\n",
    "    md_ar1 = []\n",
    "    id_ar1 = []\n",
    "\n",
    "    nmasses = df1['massCalib'].values\n",
    "    charges = df1['charge'].values\n",
    "    nmasses_int = df1['massCalib_int'].values\n",
    "    nmasses_int_dict = defaultdict(set)\n",
    "    for idx, nm in enumerate(nmasses_int):\n",
    "        nmasses_int_dict[nm].add(idx)\n",
    "        nmasses_int_dict[nm-1].add(idx)\n",
    "        nmasses_int_dict[nm+1].add(idx)\n",
    "\n",
    "    mz_acc_checked = set()\n",
    "\n",
    "    for mz_int in accurate_mz_map:\n",
    "        if mz_int in nmasses_int_dict:\n",
    "            for mz_acc in accurate_mz_map[mz_int]:\n",
    "                if mz_acc not in mz_acc_checked:\n",
    "                    mz_acc_checked.add(mz_acc)\n",
    "                    for idx_nm in nmasses_int_dict[mz_int]:\n",
    "                        nm_val = nmasses[idx_nm]\n",
    "                        mass_diff_ppm = (mz_acc - nm_val) / mz_acc * 1e6\n",
    "#                         if abs(mass_diff_ppm) <= mass_accuracy:\n",
    "                        if custom_range_mass_accuracy[0] <= mass_diff_ppm <= custom_range_mass_accuracy[1]:\n",
    "#                                 md_ar1.append(mass_diff_ppm)\n",
    "                            prots_spc.update(prot_sets[mz_acc])\n",
    "                            for prot in prot_sets[mz_acc]:\n",
    "                                md_ar1.append(mass_diff_ppm)\n",
    "                                id_ar1.append(prot)\n",
    "                            break\n",
    "\n",
    "\n",
    "    prefix = 'DECOY_'\n",
    "    isdecoy = lambda x: x[0].startswith(prefix)\n",
    "    isdecoy_key_str = lambda x: x.startswith(prefix)\n",
    "    isdecoy_key = lambda x: x < 0\n",
    "    escore = lambda x: -x[1]\n",
    "\n",
    "    top100decoy_score = [prots_spc.get(dprot, 0) for dprot in protsN]\n",
    "    top100decoy_N = [val for key, val in protsN.items()]\n",
    "    p = np.mean(top100decoy_score) / np.mean(top100decoy_N)\n",
    "    print('p=%s' % (np.mean(top100decoy_score) / np.mean(top100decoy_N)))\n",
    "\n",
    "    names_arr = np.array(list(prots_spc.keys()))\n",
    "    v_arr = np.array([prots_spc[k] for k in names_arr])\n",
    "    n_arr = np.array([protsN[k] for k in names_arr])\n",
    "\n",
    "    prots_spc2 = dict()\n",
    "    all_pvals = calc_sf_all(v_arr, n_arr, p)\n",
    "    for idx, k in enumerate(names_arr):\n",
    "        prots_spc2[k] = all_pvals[idx]   \n",
    "\n",
    "\n",
    "    cnt = Counter()\n",
    "    for k, v in prots_spc2.items():\n",
    "        if v >= score_threshold:\n",
    "            sp_id = cnt_to_spec[k-1]\n",
    "            cnt[sp_id] += 1\n",
    "\n",
    "\n",
    "    top_5_k = set()\n",
    "    for k, v in cnt.most_common():\n",
    "        if len(top_5_k) < 5:\n",
    "            top_5_k.add(k)\n",
    "\n",
    "            \n",
    "    return cnt, top_5_k, md_ar1, id_ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to folder with raw files\n",
    "infolder = '/home/kae-13-1/bact_Zgoda_Mar2023/2024-02-28_uniprot_search_reduced/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next step, you should install ThermoRawFileParser (https://github.com/compomics/ThermoRawFileParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ThermoRawFileParser for all raw files with an option to extract only MS1 spectra\n",
    "\n",
    "for fn in os.listdir(infolder):\n",
    "    if fn.endswith('.raw'):\n",
    "        infile1 = os.path.join(infolder, fn)\n",
    "        !ThermoRawFileParser -i $infile1 -L 1 -o $infolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next step, you should install biosaur2 (https://github.com/markmipt/biosaur2)\n",
    "!pip install biosaur2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run biosaur2 for all mzML files to extract peptide isotope clusters.\n",
    "\n",
    "for fn in os.listdir(infolder):\n",
    "    if fn.endswith('.mzML'):\n",
    "        mzmlname = os.path.join(infolder, fn)\n",
    "        !biosaur2 $mzmlname -minlh 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229db4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb186b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_files =  0\n",
    "for z in listdir(infolder):\n",
    "    if (z.endswith('features.tsv')):\n",
    "        print(z)\n",
    "        n_files+=1\n",
    "#         break\n",
    "        df1 = pd.read_table(path.join(infolder, z))\n",
    "        df1 = df1[df1['nIsotopes'] >= 3]\n",
    "        df1 = df1[df1['charge'] >= 2]\n",
    "        df1 = df1[df1['charge'] <= 3]\n",
    "        df1['massCalib_int'] = df1['massCalib'].apply(lambda x: int(x/mz_step))\n",
    "        print(len(df1))\n",
    "        \n",
    "        cnt, top_5_k, md_ar1, id_ar1 = get_matches(df1, [-mass_accuracy, mass_accuracy], score_threshold=4.0)         \n",
    "            \n",
    "        md_ar2 = []\n",
    "        for z1, z2 in zip(md_ar1, id_ar1):\n",
    "            if cnt_to_spec[z2-1] in top_5_k:\n",
    "                md_ar2.append(z1)\n",
    "\n",
    "        from scipy.optimize import curve_fit\n",
    "\n",
    "        def noisygaus(x, a, x0, sigma, b):\n",
    "            return a * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2)) + b\n",
    "\n",
    "\n",
    "        md_ar2 = np.array(md_ar2)\n",
    "        bbins = np.arange(min(md_ar2), max(md_ar2), 0.1)\n",
    "        H2, b2 = np.histogram(md_ar2, bins=bbins)\n",
    "        m, mi, s = max(H2), b2[np.argmax(H2)], (max(md_ar2) - min(md_ar2))/6\n",
    "        noise = min(H2)\n",
    "        popt, pcov = curve_fit(noisygaus, b2[1:], H2, p0=[m, mi, s, noise])\n",
    "        shift, sigma = popt[1], abs(popt[2])\n",
    "        print('Optimized mass shift and sigma: ', shift, sigma)\n",
    "\n",
    "        custom_range_mass_accuracy = [shift-2*sigma, shift+2*sigma]\n",
    "        \n",
    "        cnt, _, md_ar1, id_ar1 = get_matches(df1, custom_range_mass_accuracy, score_threshold=4.0)    \n",
    "            \n",
    "            \n",
    "        number_of_top_proteins = 15\n",
    "    \n",
    "        top_100_species_names = set()\n",
    "        for k, v in cnt.most_common():\n",
    "            if len(top_100_species_names) < number_of_top_proteins:\n",
    "                k_orig = spec_map_id_reversed[k]\n",
    "                if len_uniprot[int(k_orig)] < 220000:\n",
    "                    if not int(k_orig) in exclude_names: ############################################################\n",
    "                        top_100_species_names.add(k_orig) # OR k_orig???\n",
    "                        orig_name = ncbi.get_taxid_translator([k_orig,])[int(k_orig)]\n",
    "                        print(k, k_orig, orig_name, v)\n",
    "            \n",
    "        prots = []\n",
    "        cnt = 0\n",
    "        \n",
    "        report = pd.DataFrame()\n",
    "        \n",
    "        for leader in top_100_species_names:\n",
    "            SP = 0\n",
    "            UN = 0\n",
    "            \n",
    "            if str(leader)+'.fasta' in listdir(path_to_fasta_dir_sprot):\n",
    "                for p in fasta.read(path.join(path_to_fasta_dir_sprot, str(leader)+'.fasta')):\n",
    "                    SP+=1\n",
    "            for p in fasta.read(path.join(path_to_fasta_dir_uniprot, str(leader)+'.fasta')):\n",
    "                prots.append(p)\n",
    "                UN+=1\n",
    "            report = pd.concat([report, pd.DataFrame.from_dict({'ID':[leader],\n",
    "                                                               'Sprot':[SP],\n",
    "                                                               'Uniprot':[UN]})])\n",
    "            \n",
    "        \n",
    "        report.to_csv(path.join(infolder, z.replace('.features.tsv', '.strain_statistics.tsv')))\n",
    "            \n",
    "        random.shuffle(prots)        \n",
    "        fasta.write(prots, output=open(path.join(infolder, z.replace('.features.tsv', '_top15_leaders_15.fasta')), 'w')).close()\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next step, you should install ms1searchpy (https://github.com/markmipt/ms1searchpy)\n",
    "!pip install ms1searchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae53a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next step, you should install DeepLC (https://github.com/compomics/DeepLC)\n",
    "# The recommended version is the clone available at https://github.com/markmipt/DeepLC\n",
    "# The latest official DeepLC versions should work too, but ms1searchpy processing time will be much longer\n",
    "!pip install https://github.com/markmipt/DeepLC/archive/refs/heads/alternative_best_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43d568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run standard DirectMS1 analysis\n",
    "for file in listdir(infolder):\n",
    "    if (file.endswith('features.tsv')) and (file.replace('.tsv', '_proteins.tsv') not in listdir(infolder)):\n",
    "        print(file)\n",
    "        file_name = file.split('.features')[0]\n",
    "        fasta_name = file_name + '_top15_leaders_15.fasta'\n",
    "        fasta = path.join(infolder, fasta_name)\n",
    "        file = path.join(infolder, file)\n",
    "        !ms1searchpy $file -d $fasta -sc 3 -i 2 -nproc 10 -mc 0 -ad 1\\\n",
    "        -cmin 1 -ptol 8 -fdr 5 -ts 2 -ml 1\\\n",
    "        -deeplc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc866d9",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
